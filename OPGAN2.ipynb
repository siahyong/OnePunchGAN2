{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OPGAN2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_1xBtl67oiC"
      },
      "source": [
        "To use this notebook, just run it from top to bottom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qzqv9V58fvL"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ugEZgAS8DWe"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "import torch.nn.functional as F\r\n",
        "import glob as glob\r\n",
        "from tqdm.auto import tqdm\r\n",
        "from torchvision import transforms\r\n",
        "from torchvision.utils import make_grid\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import torchvision\r\n",
        "from skimage import color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5RaQwq96_zj"
      },
      "source": [
        "Model Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq-C8LO09Ap_"
      },
      "source": [
        "class ContractingBlock(nn.Module):\r\n",
        "  def __init__(self, input_channels, use_bn=True):\r\n",
        "    super(ContractingBlock, self).__init__()\r\n",
        "    self.conv1 = nn.Conv2d(input_channels, input_channels*2, kernel_size=3, padding=1)\r\n",
        "    self.conv2 = nn.Conv2d(input_channels*2, input_channels*2, kernel_size=3, padding=1)\r\n",
        "    self.activation = nn.LeakyReLU(0.2)\r\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "    if use_bn:\r\n",
        "      self.batchnorm = nn.BatchNorm2d(input_channels * 2)\r\n",
        "    self.use_bn = use_bn\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    if self.use_bn:\r\n",
        "      x = self.batchnorm(x)\r\n",
        "    x = self.activation(x)\r\n",
        "    x = self.conv2(x)\r\n",
        "    if self.use_bn:\r\n",
        "      x = self.batchnorm(x)\r\n",
        "    x = self.activation(x)\r\n",
        "    x = self.maxpool(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "def crop(image, new_shape):\r\n",
        "  cropped_image = image[:,:,int((image.shape[2]-new_shape[2])/2):int((image.shape[2]+new_shape[2])/2),int((image.shape[3]-new_shape[3])/2):int((image.shape[3]+new_shape[3])/2)]\r\n",
        "  return cropped_image\r\n",
        "\r\n",
        "class ExpandingBlock(nn.Module):\r\n",
        "  def __init__(self, input_channels, use_bn = True):\r\n",
        "    super(ExpandingBlock, self).__init__()\r\n",
        "    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\r\n",
        "    self.conv1 = nn.Conv2d(input_channels, int(input_channels/2), kernel_size=2)\r\n",
        "    self.conv2 = nn.Conv2d(input_channels, int(input_channels/2), kernel_size=3, padding=1)\r\n",
        "    self.conv3 = nn.Conv2d(int(input_channels/2), int(input_channels/2), kernel_size=2, padding = 1)\r\n",
        "    if use_bn:\r\n",
        "      self.batchnorm = nn.BatchNorm2d(input_channels // 2)\r\n",
        "      self.use_bn = use_bn\r\n",
        "    self.activation = nn.ReLU()\r\n",
        "\r\n",
        "  def forward(self, x, skip_con_x):\r\n",
        "    x = self.upsample(x)\r\n",
        "    x = self.conv1(x)\r\n",
        "    skip_con_x = crop(skip_con_x, x.shape)\r\n",
        "    x = torch.cat([x, skip_con_x], axis=1)\r\n",
        "    x = self.conv2(x)\r\n",
        "    if self.use_bn:\r\n",
        "      x = self.batchnorm(x)\r\n",
        "    x = self.activation(x)\r\n",
        "    x = self.conv3(x)\r\n",
        "    if self.use_bn:\r\n",
        "      x = self.batchnorm(x)\r\n",
        "    x = self.activation(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "class FeatureMapBlock(nn.Module):\r\n",
        "    def __init__(self, input_channels, output_channels):\r\n",
        "        super(FeatureMapBlock, self).__init__()\r\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=1)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.conv(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "class Generator(nn.Module):\r\n",
        "  def __init__(self, input_channels, output_channels, hidden_channels=64):\r\n",
        "    super(Generator, self).__init__()\r\n",
        "    self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\r\n",
        "    self.contract1 = ContractingBlock(hidden_channels)\r\n",
        "    self.contract2 = ContractingBlock(hidden_channels * 2)\r\n",
        "    self.contract3 = ContractingBlock(hidden_channels * 4)\r\n",
        "    self.contract4 = ContractingBlock(hidden_channels * 8)\r\n",
        "    self.expand1 = ExpandingBlock(hidden_channels * 16)\r\n",
        "    self.expand2 = ExpandingBlock(hidden_channels * 8)\r\n",
        "    self.expand3 = ExpandingBlock(hidden_channels * 4)\r\n",
        "    self.expand4 = ExpandingBlock(hidden_channels * 2)\r\n",
        "    self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\r\n",
        "    self.sigmoid = torch.nn.Sigmoid()\r\n",
        "  \r\n",
        "  def forward(self,x):\r\n",
        "    x0 = self.upfeature(x)\r\n",
        "    x1 = self.contract1(x0)\r\n",
        "    x2 = self.contract2(x1)\r\n",
        "    x3 = self.contract3(x2)\r\n",
        "    x4 = self.contract4(x3)\r\n",
        "    x5 = self.expand1(x4, x3)\r\n",
        "    x6 = self.expand2(x5, x2)\r\n",
        "    x7 = self.expand3(x6, x1)\r\n",
        "    x8 = self.expand4(x7, x0)\r\n",
        "    xn = self.downfeature(x8)\r\n",
        "    return self.sigmoid(xn)\r\n",
        "\r\n",
        "class Discriminator(nn.Module):\r\n",
        "    '''\r\n",
        "    Discriminator Class\r\n",
        "    Structured like the contracting path of the U-Net, the discriminator will\r\n",
        "    output a matrix of values classifying corresponding portions of the image as real or fake. \r\n",
        "    Parameters:\r\n",
        "        input_channels: the number of image input channels\r\n",
        "        hidden_channels: the initial number of discriminator convolutional filters\r\n",
        "    '''\r\n",
        "    def __init__(self, input_channels, hidden_channels=8):\r\n",
        "        super(Discriminator, self).__init__()\r\n",
        "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\r\n",
        "        self.contract1 = ContractingBlock(hidden_channels, use_bn=False)\r\n",
        "        self.contract2 = ContractingBlock(hidden_channels * 2)\r\n",
        "        self.contract3 = ContractingBlock(hidden_channels * 4)\r\n",
        "        self.contract4 = ContractingBlock(hidden_channels * 8)\r\n",
        "        #### START CODE HERE ####\r\n",
        "        self.final = nn.Conv2d(hidden_channels * 16, 1, kernel_size=1)\r\n",
        "        #### END CODE HERE ####\r\n",
        "\r\n",
        "    def forward(self, x, y):\r\n",
        "        x = torch.cat([x, y], axis=1)\r\n",
        "        x0 = self.upfeature(x)\r\n",
        "        x1 = self.contract1(x0)\r\n",
        "        x2 = self.contract2(x1)\r\n",
        "        x3 = self.contract3(x2)\r\n",
        "        x4 = self.contract4(x3)\r\n",
        "        xn = self.final(x4)\r\n",
        "        return xn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6WgPpFm67tW"
      },
      "source": [
        "Mount your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ19OJMtQOjF",
        "outputId": "72b860ef-6745-4f4a-81ff-ae6bf939b339"
      },
      "source": [
        "# Connect Google Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Google Drive connected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv27RDauPaAA"
      },
      "source": [
        "#@markdown ## Input File\r\n",
        "Input_File = \"CS236G_Project/DAIN_test/test.mkv\" #@param{type:\"string\"}\r\n",
        "\r\n",
        "#@markdown ## Frame Location\r\n",
        "Frame_Storage = \"CS236G_Project/test_frames\" #@param{type:\"string\"}\r\n",
        "\r\n",
        "#@markdown ## Chunk Location\r\n",
        "Chunk_Storage = \"CS236G_Project/testing_chunks\" #@param{type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NuKwgDK6wyJ"
      },
      "source": [
        "Example code converting video input into frame data shown below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frwPSK8w6brm"
      },
      "source": [
        "%shell ffmpeg -i '/content/gdrive/MyDrive/{Input_File}' -vf 'select=gte(n\\,1),setpts=PTS-STARTPTS,scale=704:480' '/content/gdrive/MyDrive/{Frame_Storage}/default/%05d.png'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNrgGf9A7JfD"
      },
      "source": [
        "Parameter Setting, note, when device is set to 'cuda', you need to be on GPU for the notebook to run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWTorLlyq7vn"
      },
      "source": [
        "# New parameters\r\n",
        "adv_criterion = nn.BCEWithLogitsLoss() \r\n",
        "recon_criterion = nn.L1Loss() \r\n",
        "lambda_recon = 200\r\n",
        "\r\n",
        "n_epochs = 2\r\n",
        "input_dim = 6\r\n",
        "real_dim = 3\r\n",
        "display_step = 500\r\n",
        "batch_size = 1\r\n",
        "lr = 0.0002\r\n",
        "target_shape = 256\r\n",
        "device = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQujoVLW7Moy"
      },
      "source": [
        "Build Generator and Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtUNW1ncrmih"
      },
      "source": [
        "gen = Generator(input_dim, real_dim).to(device)\r\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\r\n",
        "disc = Discriminator(input_dim + real_dim).to(device)\r\n",
        "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elB_8CrZ7ZmS"
      },
      "source": [
        "Fill in your checkpoint location below if you want to train from checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgumMt47xfMf"
      },
      "source": [
        "# Your checkpoint location goes here\r\n",
        "save_file_location = \"/content/gdrive/MyDrive/CS236G_Project/opgan2_14000.pth\"\r\n",
        "pretrained = True\r\n",
        "\r\n",
        "def weights_init(m):\r\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\r\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n",
        "    if isinstance(m, nn.BatchNorm2d):\r\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n",
        "        torch.nn.init.constant_(m.bias, 0)\r\n",
        "\r\n",
        "if pretrained:\r\n",
        "    loaded_state = torch.load(save_file_location)\r\n",
        "    gen.load_state_dict(loaded_state[\"gen\"])\r\n",
        "    gen_opt.load_state_dict(loaded_state[\"gen_opt\"])\r\n",
        "    disc.load_state_dict(loaded_state[\"disc\"])\r\n",
        "    disc_opt.load_state_dict(loaded_state[\"disc_opt\"])\r\n",
        "else:\r\n",
        "    gen = gen.apply(weights_init)\r\n",
        "    disc = disc.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fso8PyD7yQz"
      },
      "source": [
        "Define Generator Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtxeEXgJxj_G"
      },
      "source": [
        "def get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon):\r\n",
        "    '''\r\n",
        "    Return the loss of the generator given inputs.\r\n",
        "    Parameters:\r\n",
        "        gen: the generator; takes the condition and returns potential images\r\n",
        "        disc: the discriminator; takes images and the condition and\r\n",
        "          returns real/fake prediction matrices\r\n",
        "        real: the real images (e.g. maps) to be used to evaluate the reconstruction\r\n",
        "        condition: the source images (e.g. satellite imagery) which are used to produce the real images\r\n",
        "        adv_criterion: the adversarial loss function; takes the discriminator \r\n",
        "                  predictions and the true labels and returns a adversarial \r\n",
        "                  loss (which you aim to minimize)\r\n",
        "        recon_criterion: the reconstruction loss function; takes the generator \r\n",
        "                    outputs and the real images and returns a reconstructuion \r\n",
        "                    loss (which you aim to minimize)\r\n",
        "        lambda_recon: the degree to which the reconstruction loss should be weighted in the sum\r\n",
        "    '''\r\n",
        "    fake = gen(condition)\r\n",
        "    disc_label = disc(fake, condition)\r\n",
        "    adv_loss = adv_criterion(disc_label, torch.ones_like(disc_label))\r\n",
        "    recon_loss = recon_criterion(fake, real)\r\n",
        "    gen_loss = adv_loss + lambda_recon * recon_loss\r\n",
        "    return gen_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "295rY2xA74vT"
      },
      "source": [
        "Define function that converts frames to chunks and saves them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_42MDTi4O8I"
      },
      "source": [
        "def image2chunk(folder,image_index, frame_gap = 1):\r\n",
        "  a_name = '/content/gdrive/MyDrive/{}/{}/{}.png'.format(Frame_Storage, folder, f'{image_index:05}')\r\n",
        "  b_name = '/content/gdrive/MyDrive/{}/{}/{}.png'.format(Frame_Storage, folder, f'{(image_index + frame_gap):05}')\r\n",
        "  c_name = '/content/gdrive/MyDrive/{}/{}/{}.png'.format(Frame_Storage, folder, f'{(image_index + 2*frame_gap):05}')\r\n",
        "  # print(a_name)\r\n",
        "  a = np.array(Image.open(a_name))/255\r\n",
        "  b = np.array(Image.open(b_name))/255\r\n",
        "  c = np.array(Image.open(c_name))/255\r\n",
        "  chunk = np.concatenate((a,c,b), axis = 1)\r\n",
        "  chunk_im = Image.fromarray((chunk * 255).astype(np.uint8))\r\n",
        "  chunk_im.save('/content/gdrive/MyDrive/{}/{}/{}.png'.format(Chunk_Storage, folder, f'{image_index:05}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NmbMPxX8Ebk"
      },
      "source": [
        "This code box will search through the listed subfolders of the folder you specify, convert the frames into chunks, then save the chunks in the corresponding subfolder in the Chunk Storage folder you specified previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6uBJHlXL8FL"
      },
      "source": [
        "frame_gap = 1\r\n",
        "folders = ['a', 'b', 'c1', 'c2']\r\n",
        "\r\n",
        "for folder in folders:\r\n",
        "  frame_count = len(list(glob.iglob(\"/content/gdrive/MyDrive/{}/{}/*.png\".format(Frame_Storage, folder))))\r\n",
        "  todo = frame_count - 2*frame_gap\r\n",
        "  print(\"Commencing on frame folder {}\".format(folder))\r\n",
        "  for i in range(1, todo+1):\r\n",
        "    image2chunk(folder, i, frame_gap)\r\n",
        "    if i%50 == 0:\r\n",
        "      print(\"Processed {} out of {} on todo list\".format(i, todo))\r\n",
        "  print(\"Finished on frame folder {}\".format(folder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvS6WoWHZlB5"
      },
      "source": [
        "Visualization function\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15f2sVlwZ4aa"
      },
      "source": [
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\r\n",
        "    '''\r\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\r\n",
        "    size per image, plots and prints the images in an uniform grid.\r\n",
        "    '''\r\n",
        "    image_shifted = image_tensor\r\n",
        "    image_unflat = image_shifted.detach().cpu().view(-1, *size)\r\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\r\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMjY37sm86Sk"
      },
      "source": [
        "Define some needed transforms for later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fItaZe9hZN74"
      },
      "source": [
        "transform = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWd69-uZ89NK"
      },
      "source": [
        "Create the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL8ObgoC_Wwi"
      },
      "source": [
        "# Replace Chunk_Storage with wherever you saved the actual training chunks\r\n",
        "training_chunk_storage = Chunk_Storage\r\n",
        "\r\n",
        "dataset = torchvision.datasets.ImageFolder('/content/gdrive/MyDrive/{}'.format(training_chunk_storage), transform=transform)\r\n",
        "dataset_len = len(dataset)\r\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [dataset_len - dataset_len//5, dataset_len//5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwb7AaTj9f3Z"
      },
      "source": [
        "Create the testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fla8vnS9fLy"
      },
      "source": [
        "test_dataset = torchvision.datasets.ImageFolder('/content/gdrive/MyDrive/{}'.format(Chunk_Storage), transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L87vfJQ9pjC"
      },
      "source": [
        "Define the training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCWf2ikPy6yG"
      },
      "source": [
        "train_gen = []\r\n",
        "train_disc = []\r\n",
        "val_gen = []\r\n",
        "val_disc = []\r\n",
        "\r\n",
        "# How often this model saves. You can modify it, as a heads up, each checkpoint takes up like 300 MB of space\r\n",
        "save_rate = 2000\r\n",
        "\r\n",
        "# Folder where checkpoints are saved, feel free to replace with whatever you want\r\n",
        "save_location = \"CS236G_Project\"\r\n",
        "\r\n",
        "def train(save_model=False):\r\n",
        "    mean_generator_loss = 0\r\n",
        "    mean_discriminator_loss = 0\r\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\r\n",
        "    cur_step = 0\r\n",
        "    val_count_limit = 100\r\n",
        "\r\n",
        "    for epoch in range(n_epochs):\r\n",
        "        # Dataloader returns the batches\r\n",
        "        for image, _ in tqdm(train_dataloader):\r\n",
        "            #image_width = image.shape[3]\r\n",
        "            #condition = image[:, :, :, :image_width // 2]\r\n",
        "            #condition = nn.functional.interpolate(condition, size=target_shape)\r\n",
        "            #real = image[:, :, :, image_width // 2:]\r\n",
        "            #real = nn.functional.interpolate(real, size=target_shape)\r\n",
        "\r\n",
        "            image_width = image.shape[3]\r\n",
        "            pre = image[:, :, :, :image_width // 3]\r\n",
        "            post = image[:, :, :, image_width // 3:2*image_width // 3]\r\n",
        "            condition = torch.cat((pre, post), dim=1)\r\n",
        "            real = image[:, :, :, 2*image_width // 3:]\r\n",
        "\r\n",
        "            cur_batch_size = len(condition)\r\n",
        "            condition = condition.to(device)\r\n",
        "            real = real.to(device)\r\n",
        "\r\n",
        "            ### Update discriminator ###\r\n",
        "            disc_opt.zero_grad() # Zero out the gradient before backpropagation\r\n",
        "            with torch.no_grad():\r\n",
        "                fake = gen(condition)\r\n",
        "            disc_fake_hat = disc(fake.detach(), condition) # Detach generator\r\n",
        "            disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))\r\n",
        "            disc_real_hat = disc(real, condition)\r\n",
        "            disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))\r\n",
        "            disc_loss = (disc_fake_loss + disc_real_loss) / 2\r\n",
        "            disc_loss.backward(retain_graph=True) # Update gradients\r\n",
        "            disc_opt.step() # Update optimizer\r\n",
        "\r\n",
        "            ### Update generator ###\r\n",
        "            gen_opt.zero_grad()\r\n",
        "            gen_loss = get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon)\r\n",
        "            gen_loss.backward() # Update gradients\r\n",
        "            gen_opt.step() # Update optimizer\r\n",
        "\r\n",
        "            # Keep track of the average discriminator loss\r\n",
        "            mean_discriminator_loss += disc_loss.item() / display_step\r\n",
        "            # Keep track of the average generator loss\r\n",
        "            mean_generator_loss += gen_loss.item() / display_step\r\n",
        "\r\n",
        "            ### Visualization code ###\r\n",
        "            if cur_step % display_step == 0:\r\n",
        "                if cur_step > 0:\r\n",
        "                    print(f\"Epoch {epoch}: Step {cur_step}: Generator (U-Net) loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\r\n",
        "                    train_gen.append(mean_generator_loss)\r\n",
        "                    train_disc.append(mean_discriminator_loss)\r\n",
        "                else:\r\n",
        "                    print(\"Pretrained initial state\")\r\n",
        "                #show_tensor_images(condition, size=(input_dim, target_shape, target_shape))\r\n",
        "                show_tensor_images(real, size=(3, 480, 704))\r\n",
        "                show_tensor_images(fake, size=(3, 480, 704))\r\n",
        "\r\n",
        "                mean_generator_loss = 0\r\n",
        "                mean_discriminator_loss = 0\r\n",
        "\r\n",
        "                val_count = 0\r\n",
        "                val_mean_gen_loss = 0\r\n",
        "                val_mean_disc_loss = 0\r\n",
        "                for image, _ in tqdm(val_dataloader):\r\n",
        "                  image_width = image.shape[3]\r\n",
        "                  pre = image[:, :, :, :image_width // 3]\r\n",
        "                  post = image[:, :, :, image_width // 3:2*image_width // 3]\r\n",
        "                  condition = torch.cat((pre, post), dim=1)\r\n",
        "                  real = image[:, :, :, 2*image_width // 3:]\r\n",
        "\r\n",
        "                  cur_batch_size = len(condition)\r\n",
        "                  condition = condition.to(device)\r\n",
        "                  real = real.to(device)\r\n",
        "\r\n",
        "                  disc_opt.zero_grad() # Zero out the gradient before backpropagation\r\n",
        "                  with torch.no_grad():\r\n",
        "                      fake = gen(condition)\r\n",
        "                  disc_fake_hat = disc(fake.detach(), condition) # Detach generator\r\n",
        "                  disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))\r\n",
        "                  disc_real_hat = disc(real, condition)\r\n",
        "                  disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))\r\n",
        "                  disc_loss = (disc_fake_loss + disc_real_loss) / 2\r\n",
        "                  #disc_loss.backward(retain_graph=True) # Update gradients\r\n",
        "                  #disc_opt.step() # Update optimizer\r\n",
        "\r\n",
        "                  ### Update generator ###\r\n",
        "                  gen_opt.zero_grad()\r\n",
        "                  gen_loss = get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon)\r\n",
        "                  #gen_loss.backward() # Update gradients\r\n",
        "                  #gen_opt.step() # Update optimizer\r\n",
        "\r\n",
        "                  val_mean_gen_loss += gen_loss.item() / val_count_limit\r\n",
        "                  val_mean_disc_loss += disc_loss.item() / val_count_limit\r\n",
        "                  \r\n",
        "                  val_count += 1\r\n",
        "                  if val_count >= val_count_limit:\r\n",
        "                    break\r\n",
        "                print(\"Validation Set Gen Loss: {}, Validation Set Disc Loss: {}\".format(val_mean_gen_loss, val_mean_disc_loss))\r\n",
        "                val_gen.append(val_mean_gen_loss)\r\n",
        "                val_disc.append(val_mean_disc_loss)\r\n",
        "\r\n",
        "                # You can change save_model to True if you'd like to save the model\r\n",
        "            if cur_step % save_rate == 0:\r\n",
        "                if save_model:\r\n",
        "                    torch.save({'gen': gen.state_dict(),\r\n",
        "                        'gen_opt': gen_opt.state_dict(),\r\n",
        "                        'disc': disc.state_dict(),\r\n",
        "                        'disc_opt': disc_opt.state_dict()\r\n",
        "                    }, f\"/content/gdrive/MyDrive/{save_location}/opgan2_{cur_step}.pth\")\r\n",
        "            cur_step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arMrVR84-8PB"
      },
      "source": [
        "Define the testing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTqYIFDXvO94"
      },
      "source": [
        "def test_generator(test_count_limit = 300):\r\n",
        "  test_count = 0\r\n",
        "  test_mean_gen_loss = 0\r\n",
        "  test_mean_disc_loss = 0\r\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\r\n",
        "  for image, _ in tqdm(test_dataloader):\r\n",
        "    image_width = image.shape[3]\r\n",
        "    pre = image[:, :, :, :image_width // 3]\r\n",
        "    post = image[:, :, :, image_width // 3:2*image_width // 3]\r\n",
        "    condition = torch.cat((pre, post), dim=1)\r\n",
        "    real = image[:, :, :, 2*image_width // 3:]\r\n",
        "\r\n",
        "    cur_batch_size = len(condition)\r\n",
        "    condition = condition.to(device)\r\n",
        "    real = real.to(device)\r\n",
        "\r\n",
        "    disc_opt.zero_grad() # Zero out the gradient before backpropagation\r\n",
        "    with torch.no_grad():\r\n",
        "        fake = gen(condition)\r\n",
        "    disc_fake_hat = disc(fake.detach(), condition) # Detach generator\r\n",
        "    disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))\r\n",
        "    disc_real_hat = disc(real, condition)\r\n",
        "    disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))\r\n",
        "    disc_loss = (disc_fake_loss + disc_real_loss) / 2\r\n",
        "\r\n",
        "    gen_opt.zero_grad()\r\n",
        "    gen_loss = get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon)\r\n",
        "\r\n",
        "    test_mean_gen_loss += gen_loss.item() / test_count_limit\r\n",
        "    test_mean_disc_loss += disc_loss.item() / test_count_limit\r\n",
        "\r\n",
        "    if test_count % 10 == 0:\r\n",
        "      show_tensor_images(real, size=(3, 480, 704))\r\n",
        "      show_tensor_images(fake, size=(3, 480, 704))\r\n",
        "\r\n",
        "    test_count += 1\r\n",
        "    if test_count >= test_count_limit:\r\n",
        "      break\r\n",
        "  print(\"Test Set Gen Loss: {}, Test Set Disc Loss: {}\".format(test_mean_gen_loss, test_mean_disc_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWgbDgW9_Arx"
      },
      "source": [
        "Run the training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GetB0BRS-_6F"
      },
      "source": [
        "train(save_model=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzsKEBJU-bXk"
      },
      "source": [
        "Plot Generator loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm9sXWP1mriq"
      },
      "source": [
        "train_gen_spaced = np.zeros((len(train_gen), 2))\r\n",
        "for i in range(len(train_gen)):\r\n",
        "  train_gen_spaced[i,0] = (i+1)*500\r\n",
        "  train_gen_spaced[i,1] = train_gen[i]\r\n",
        "\r\n",
        "val_gen_spaced = np.zeros((len(val_gen), 2))\r\n",
        "for i in range(len(val_gen)):\r\n",
        "  val_gen_spaced[i,0] = i*500\r\n",
        "  val_gen_spaced[i,1] = val_gen[i]\r\n",
        "\r\n",
        "plt.plot(train_gen_spaced[:,0], train_gen_spaced[:,1])\r\n",
        "plt.plot(val_gen_spaced[:,0], val_gen_spaced[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJj_zSqy-eOD"
      },
      "source": [
        "Plot Discriminator loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84qkGzY2oT4C"
      },
      "source": [
        "train_disc_spaced = np.zeros((len(train_disc), 2))\r\n",
        "for i in range(len(train_disc)):\r\n",
        "  train_disc_spaced[i,0] = (i+1)*500\r\n",
        "  train_disc_spaced[i,1] = train_disc[i]\r\n",
        "\r\n",
        "val_disc_spaced = np.zeros((len(val_disc), 2))\r\n",
        "for i in range(len(val_disc)):\r\n",
        "  val_disc_spaced[i,0] = i*500\r\n",
        "  val_disc_spaced[i,1] = val_disc[i]\r\n",
        "\r\n",
        "plt.plot(train_disc_spaced[:,0], train_disc_spaced[:,1])\r\n",
        "plt.plot(val_disc_spaced[:,0], val_disc_spaced[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWHQoKGJ_DkJ"
      },
      "source": [
        "Run the testing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_odYQgmLxEPf"
      },
      "source": [
        "test_generator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrB5Hp3F-o7i"
      },
      "source": [
        "Experimental: Load in a checkpoint and use it to do sample interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGX6-ZMwkDxj"
      },
      "source": [
        "loaded_state = torch.load(\"/content/gdrive/MyDrive/CS236G_Project/opgan_1000.pth\")\r\n",
        "gen.load_state_dict(loaded_state[\"gen\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBD7dYULfqDr"
      },
      "source": [
        "def synthesizeframe(frame1, frame2):\r\n",
        "  frame1 = np.expand_dims(frame1, axis = 0)\r\n",
        "  frame2 = np.expand_dims(frame2, axis = 0)\r\n",
        "  ba = torch.FloatTensor(np.transpose(frame1, (0, 3, 1, 2)))\r\n",
        "  aa = torch.FloatTensor(np.transpose(frame2, (0, 3, 1, 2)))\r\n",
        "  test_input = torch.cat((ba, aa), dim=1).to(device)\r\n",
        "  output = gen(test_input)\r\n",
        "  show_tensor_images(ba, size=(3, 480, 704))\r\n",
        "  show_tensor_images(output, size=(3, 480, 704))\r\n",
        "  show_tensor_images(aa, size=(3, 480, 704))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pJgteYZnDCE"
      },
      "source": [
        "def demo_interpolate(folder, image_index, frame_gap = 2,saveframe=False):\r\n",
        "  frame1 = np.array(Image.open('/content/gdrive/MyDrive/{}/{}/{}.png'.format(Frame_Storage, folder, f'{image_index:05}')))/255\r\n",
        "  frame2 = np.array(Image.open('/content/gdrive/MyDrive/{}/{}/{}.png'.format(Frame_Storage, folder, f'{image_index+frame_gap:05}')))/255\r\n",
        "  synthesizeframe(frame1, frame2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MCc9QzQngs7"
      },
      "source": [
        "demo_interpolate('c2', 236, frame_gap=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}